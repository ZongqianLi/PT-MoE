# PT-MoE: An Efficient Finetuning Framework for Integrating Mixture-of-Experts into Prompt Tuning

<p align="center">
  <b>Content</b>
</p>

<p align="center">
  <a href="#news">🚀 News</a> •
  <a href="#todo">✏️ Todo</a> •
  <a href="#introduction">✨ Introduction</a>
</p>

<p align="center">
  <a href="#environment">🖥️ Environment</a> •
  <a href="#inferencedemo">🤗 Inference Demo</a> •
  <a href="#finetuningdemo">🤗 Finetuning Demo</a>
</p>

<p align="center">
  <a href="#evaluation">✏️ Evaluation</a> •
  <a href="#results">🎲 Results</a>
</p>

<p align="center">
  <a href="#download">💾 Download</a> •
  <a href="#citation">📌 Citation</a> •
  <a href="#license">🔖 License</a>
</p>
<div id="news">&nbsp;</div>

<p align="center">
  <b>Links</b>
</p>

<p align="center">
  <a href="">Project Page</a> •
  <a href="">Paper</a>
</p>



## 🚀 News

- **[2025.01.24]** This page is created.

<div>&nbsp;</div>
<div>&nbsp;</div>
<div id="todo">&nbsp;</div>



## ✏️ Todo

- [ ] Upload the paper to the Arxiv.

<div>&nbsp;</div>
<div>&nbsp;</div>
<div id="introduction">&nbsp;</div>



## ✨ Introduction

### Overview performance

- 1
- 2
- 3

<p align="left">
  <img src="./figures/cover_figure.png" width="30%">
</p>

### Architecture

- **Soft prompt decomposition:** The soft prompt is decomposed into an A matrix and a B matrix.
- **Router mechanism:** The router chooses the A matrix according to the input text.

<p align="left">
  <img src="./figures/method.png" width="30%">
</p>

### Contributions

- 1
- 2
- 3

<div>&nbsp;</div>
<div>&nbsp;</div>
<div id="environment">&nbsp;</div>



## 🖥️ Environment

Please use the same environment:

```
python==3.11.5
torch==2.3.1+cu118
transformers==4.46.0
datasets==2.18.0
huggingface_hub==0.24.2
deepspeed==0.15.3
wandb==0.14.2
numpy==1.23.5
tqdm==4.66.4
```

<div>&nbsp;</div>
<div>&nbsp;</div>
<div id="inferencedemo">&nbsp;</div>



## 🤗 Inference Demo

### QA

### Math



<div>&nbsp;</div>
<div>&nbsp;</div>
<div id="finetuningdemo">&nbsp;</div>



## 🤗 Finetuning Demo

### QA

data format

codes

output

### Math



<div>&nbsp;</div>
<div>&nbsp;</div>
<div id="evaluation">&nbsp;</div>



## ✏️ Evaluation

### QA

### Math


<div>&nbsp;</div>
<div>&nbsp;</div>
<div id="results">&nbsp;</div>



## 🎲 Results

### QA

### Math


<div>&nbsp;</div>
<div>&nbsp;</div>
<div id="download">&nbsp;</div>



## 💾 Download

- [PT-MoE for QA based on llama-3.2-1b-it]()
- [PT-MoE for math based on llama-3.2-1b-it]() 

<div>&nbsp;</div>
<div>&nbsp;</div>
<div id="citation">&nbsp;</div>



## 📌 Citation

```
@misc{li2025ptmoeefficientfinetuningframework,
      title={PT-MoE: An Efficient Finetuning Framework for Integrating Mixture-of-Experts into Prompt Tuning}, 
      author={Zongqian Li and Yixuan Su and Nigel Collier},
      year={2025},
      eprint={2505.09519},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2505.09519}, 
}
```

<div>&nbsp;</div>
<div>&nbsp;</div>
<div id="license">&nbsp;</div>



## 🔖 License

```

```


