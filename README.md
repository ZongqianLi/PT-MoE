# PT-MoE: An Efficient Finetuning Framework for Integrating Mixture-of-Experts into Prompt Tuning

<p align="center">
  <b>Content</b>
</p>

<p align="center">
  <a href="#news">ğŸš€ News</a> â€¢
  <a href="#todo">âœï¸ Todo</a> â€¢
  <a href="#introduction">âœ¨ Introduction</a>
</p>

<p align="center">
  <a href="#inferencedemo">ğŸ¤— Inference Demo</a> â€¢
  <a href="#finetuningdemo">ğŸ¤— Finetuning Demo</a> â€¢
  <a href="#results">ğŸ“– Results</a>
</p>

<p align="center">
  <a href="#download">ğŸ’¾ Download</a> â€¢
  <a href="#citation">ğŸ“Œ Citation</a> â€¢
  <a href="#license">ğŸ”– License</a>
</p>
<div id="news">&nbsp;</div>



## ğŸš€ News

- **[2025.01.24]** This page is created.

<div>&nbsp;</div>
<div>&nbsp;</div>
<div id="todo">&nbsp;</div>



## âœï¸ Todo

- [ ] Upload the paper to the Arxiv.

<div>&nbsp;</div>
<div>&nbsp;</div>
<div id="introduction">&nbsp;</div>



## âœ¨ Introduction

### Overview comparison

### Architecture

### Contributions


<div>&nbsp;</div>
<div>&nbsp;</div>
<div id="inferencedemo">&nbsp;</div>



## ğŸ¤— Inference Demo

### QA

### Math



<div>&nbsp;</div>
<div>&nbsp;</div>
<div id="finetuningdemo">&nbsp;</div>



## ğŸ¤— Finetuning Demo

### QA

### Math



<div>&nbsp;</div>
<div>&nbsp;</div>
<div id="results">&nbsp;</div>



## ğŸ“– Results

### QA

### Math


<div>&nbsp;</div>
<div>&nbsp;</div>
<div id="download">&nbsp;</div>



## ğŸ’¾ Download

- [PT-MoE for QA based on llama-3.2-1b-it]()
- [PT-MoE for math based on llama-3.2-1b-it]() 

<div>&nbsp;</div>
<div>&nbsp;</div>
<div id="citation">&nbsp;</div>



## ğŸ“Œ Citation

```

```

<div>&nbsp;</div>
<div>&nbsp;</div>
<div id="license">&nbsp;</div>



## ğŸ”– License

This project is licensed under the Creative Commons Attribution 4.0 International License - see the [LICENSE](https://creativecommons.org/licenses/by/4.0/deed.en) for details.
